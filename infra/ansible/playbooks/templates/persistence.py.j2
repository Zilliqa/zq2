#!/usr/bin/env python3

import os
import sys
import json
import time
import signal
import logging
import subprocess
import threading
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict
import requests
from google.cloud import storage
from google.cloud import logging as cloud_logging

# Configuration
CONFIG = {
    'network_name': os.getenv('NETWORK_NAME', ''),
    'eth_chain_id': os.getenv('ETH_CHAIN_ID', ''),
    'backup_interval_blocks': int(os.getenv('BACKUP_INTERVAL_BLOCKS', '7200')),
    'max_backups_to_keep': int(os.getenv('MAX_BACKUPS_TO_KEEP', '50')),
    'rpc_endpoint': 'http://localhost:4202',
    'state_file': '/opt/zilliqa/last_backup_block',
    'data_dir': os.getenv('DATA_DIR', '/data'),
    'lock_file': '/var/run/zilliqa/persistence_backup.lock',
    'gcs_bucket': f"gs://{os.getenv('NETWORK_NAME', '')}-persistence",
    'log_name': f"{os.getenv('NETWORK_NAME', '')}-persistence-backup-log"
}

# Setup logging
cloud_logging_client = cloud_logging.Client()
logger = cloud_logging_client.logger(CONFIG['log_name'])

def log_message(message: str, level: str = 'INFO') -> None:
    """Log message to Google Cloud Logging."""
    log_level = getattr(logging, level.upper())
    logger.log_text(
        message, 
        severity=level,
        labels={"service": "persistence"}
    )
    print(f"[{level}] {message}")

def acquire_lock() -> bool:
    """Acquire file lock to prevent concurrent execution."""
    try:
        os.makedirs(os.path.dirname(CONFIG['lock_file']), exist_ok=True)
        with open(CONFIG['lock_file'], 'x') as f:
            f.write(str(os.getpid()))
        log_message(f"Acquired lock: {str(os.path.dirname(CONFIG['lock_file']))}")
        return True
    except FileExistsError:
        log_message(f"Lock already exists: {str(os.path.dirname(CONFIG['lock_file']))}")
        return False

def release_lock() -> None:
    """Release file lock."""
    try:
        os.remove(CONFIG['lock_file'])
    except FileNotFoundError:
        pass

def get_current_block() -> Optional[int]:
    """Get current block number from RPC endpoint."""
    try:
        response = requests.post(
            CONFIG['rpc_endpoint'],
            json={
                "jsonrpc": "2.0",
                "method": "eth_blockNumber",
                "params": [],
                "id": 1
            },
            timeout=10
        )
        response.raise_for_status()
        result = response.json()
        return int(result['result'], 16)
    except Exception as e:
        log_message(f"Failed to get current block: {str(e)}", "ERROR")
        return None

def is_node_synced() -> bool:
    """Return True if node is synced (eth_syncing result is False), else False."""
    try:
        response = requests.post(
            CONFIG['rpc_endpoint'],
            json={
                "jsonrpc": "2.0",
                "method": "eth_syncing",
                "params": [],
                "id": 1
            },
            timeout=10
        )
        response.raise_for_status()
        result = response.json()
        syncing_status = result.get('result')
        if syncing_status is False:
            return True
        return False
    except Exception as e:
        log_message(f"Failed to check node sync status: {str(e)}", "ERROR")
        return False

def get_last_backup_block() -> Optional[int]:
    """Get last backup block number from state file."""
    try:
        if os.path.exists(CONFIG['state_file']):
            with open(CONFIG['state_file'], 'r') as f:
                return int(f.read().strip())
    except Exception as e:
        log_message(f"Failed to read state file: {str(e)}", "ERROR")
    return None

def save_last_backup_block(block_number: int) -> None:
    """Save last backup block number to state file."""
    try:
        os.makedirs(os.path.dirname(CONFIG['state_file']), exist_ok=True)
        with open(CONFIG['state_file'], 'w') as f:
            f.write(str(block_number))
        log_message(f"Saved last backup block: {str(block_number)}")
    except Exception as e:
        log_message(f"Failed to save state file: {str(e)}", "ERROR")

def stop_zilliqa_service() -> bool:
    """Stop Zilliqa service."""
    try:
        subprocess.run(['sudo', 'systemctl', 'stop', 'zilliqa.service'], check=True)
        log_message(f"Stopped Zilliqa service")
        return True
    except subprocess.CalledProcessError as e:
        log_message(f"Failed to stop Zilliqa service: {str(e)}", "ERROR")
        return False

def start_zilliqa_service() -> bool:
    """Start Zilliqa service."""
    try:
        subprocess.run(['sudo', 'systemctl', 'start', 'zilliqa.service'], check=True)
        log_message(f"Started Zilliqa service")
        return True
    except subprocess.CalledProcessError as e:
        log_message(f"Failed to start Zilliqa service: {str(e)}", "ERROR")
        return False

def create_backup(block_number: int) -> Optional[str]:
    """Create backup of persistence data."""
    backup_name = f"{CONFIG['eth_chain_id']}-block-{block_number}"
    
    try:
        # Upload to GCS
        subprocess.run([
            'gsutil', '-m', 'cp', '-r',
            CONFIG['data_dir'],
            f"{CONFIG['gcs_bucket']}/{backup_name}/"
        ], check=True)
        log_message(f"Created backup: {backup_name}")
        return backup_name
    except subprocess.CalledProcessError as e:
        log_message(f"Failed to create backup: {str(e)}", "ERROR")
        return None

def cleanup_old_backups() -> None:
    """Clean up old backups from GCS bucket."""
    try:
        # List all backups
        result = subprocess.run(
            ['gsutil', 'ls', CONFIG['gcs_bucket']],
            capture_output=True,
            text=True,
            check=True
        )
        
        # Filter and sort backups
        backups = [
            line.strip() for line in result.stdout.splitlines()
            if f"{CONFIG['eth_chain_id']}-block-" in line
        ]
        backups.sort(reverse=True)
        
        # Remove old backups
        for backup in backups[CONFIG['max_backups_to_keep']:]:
            try:
                subprocess.run(['gsutil', '-m', 'rm', '-r', backup], check=True)
                log_message(f"Removed old backup: {backup}")
            except subprocess.CalledProcessError as e:
                log_message(f"Failed to remove backup {backup}: {str(e)}", "ERROR")
                
    except subprocess.CalledProcessError as e:
        log_message(f"Failed to list backups: {str(e)}", "ERROR")

def perform_backup(block_number: int) -> bool:
    """Perform the backup process."""
    start_time = time.time()
    log_message(f"Starting backup process at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} at block {block_number}")
    
    if not is_node_synced():
        log_message("Skipping backup: node is not fully synced", "WARNING")
        return False

    if not stop_zilliqa_service():
        return False
    
    backup_name = create_backup(block_number)
    if not backup_name:
        start_zilliqa_service()
        return False
    
    if not start_zilliqa_service():
        log_message("Failed to restart Zilliqa service after backup", "ERROR")
        return False
    
    cleanup_old_backups()
    
    duration = time.time() - start_time
    log_message(f"Backup completed successfully in {duration:.2f} seconds at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} at block {block_number}")
    return True

def monitor_blocks() -> None:
    """Monitor blockchain for new blocks and trigger backups."""
    while True:
        try:
            current_block = get_current_block()
            log_message(f"Current block: {str(current_block)}")
            if current_block is None:
                time.sleep(60)
                continue
                
            last_backup_block = get_last_backup_block()
            log_message(f"Last backup block: {str(last_backup_block)}")
            if last_backup_block is None:
                last_backup_block = current_block
                save_last_backup_block(last_backup_block)
            
            blocks_since_last_backup = current_block - last_backup_block
            log_message(f"Blocks since last backup: {str(blocks_since_last_backup)}")
            if blocks_since_last_backup >= CONFIG['backup_interval_blocks']:
                if acquire_lock():
                    try:
                        if perform_backup(current_block):
                            save_last_backup_block(current_block)
                    finally:
                        release_lock()
            
            time.sleep(60)
        except Exception as e:
            log_message(f"Error in block monitoring: {str(e)}", "ERROR")
            time.sleep(60)

def signal_handler(signum, frame) -> None:
    """Handle termination signals."""
    log_message("Received termination signal, shutting down...")
    release_lock()
    sys.exit(0)

def main() -> None:
    """Main entry point."""
    # Validate configuration
    if not CONFIG['network_name'] or not CONFIG['eth_chain_id']:
        log_message("Missing required environment variables: NETWORK_NAME or ETH_CHAIN_ID", "ERROR")
        sys.exit(1)
    
    # Setup signal handlers
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    
    log_message("Starting persistence backup service")
    monitor_blocks()

if __name__ == "__main__":
    main() 